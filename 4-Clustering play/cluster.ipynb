{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccdcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# CLUSTERING APRIMORADO - CAPTURANDO MÚLTIPLOS PADRÕES ALIMENTARES\n",
    "# Melhorias: Tratamento de dados binários, múltiplos métodos, validação robusta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from kneed import KneeLocator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🍎 CLUSTERING APRIMORADO - ANÁLISE DE PADRÕES ALIMENTARES INFANTIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"🎯 Objetivo: Identificar múltiplos grupos com padrões alimentares distintos\")\n",
    "print(\"📊 Melhorias: Tratamento especial para dados binários, múltiplos algoritmos\")\n",
    "print()\n",
    "\n",
    "# ====================================================================\n",
    "# 1. CARREGAR E PREPARAR DADOS\n",
    "# ====================================================================\n",
    "\n",
    "print(\"📊 1. CARREGANDO E PREPARANDO DATASET\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "df = pd.read_csv('/Users/marcelosilva/Desktop/clustering(0-4)/3-E-Aval/DSFinal.csv')\n",
    "\n",
    "print(f\"✅ Dataset original: {df.shape[0]:,} crianças × {df.shape[1]} variáveis\")\n",
    "\n",
    "# Remover variáveis de frequência e preparar dados\n",
    "frequency_vars = ['e13_fruta_vezes', 'e17_sal_vezes']\n",
    "# Verificar quais colunas realmente existem\n",
    "cols_to_remove = [col for col in frequency_vars if col in df.columns]\n",
    "if cols_to_remove:\n",
    "    df_filtered = df.drop(columns=cols_to_remove)\n",
    "else:\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "# Verificar e remover colunas string se existirem\n",
    "string_cols = ['e18_oferecida', 'e21a_pao']\n",
    "existing_string_cols = [col for col in string_cols if col in df_filtered.columns]\n",
    "if existing_string_cols:\n",
    "    df_clean = df_filtered.drop(columns=existing_string_cols)\n",
    "else:\n",
    "    df_clean = df_filtered.copy()\n",
    "\n",
    "X = df_clean.drop('id_anon', axis=1)\n",
    "ids = df_clean['id_anon']\n",
    "\n",
    "print(f\"✅ Dataset limpo: {X.shape[0]:,} crianças × {X.shape[1]} variáveis\")\n",
    "\n",
    "# Análise da natureza dos dados\n",
    "binary_count = sum((X[col].nunique() == 2) for col in X.columns)\n",
    "print(f\"📊 Variáveis binárias: {binary_count}/{X.shape[1]} ({binary_count/X.shape[1]*100:.1f}%)\")\n",
    "\n",
    "# Verificar distribuição dos dados\n",
    "print(\"\\n🔍 Análise da distribuição dos dados:\")\n",
    "prevalence = X.mean()\n",
    "print(f\"   Alimentos com prevalência > 80%: {sum(prevalence > 0.8)}\")\n",
    "print(f\"   Alimentos com prevalência < 20%: {sum(prevalence < 0.2)}\")\n",
    "print(f\"   Alimentos com prevalência 20-80%: {sum((prevalence >= 0.2) & (prevalence <= 0.8))}\")\n",
    "\n",
    "# Listar colunas disponíveis para debugging\n",
    "print(f\"\\n📋 Primeiras 10 variáveis disponíveis: {list(X.columns[:10])}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. TRANSFORMAÇÃO INTELIGENTE DOS DADOS\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n⚙️ 2. TRANSFORMAÇÃO INTELIGENTE DOS DADOS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar diferentes representações dos dados\n",
    "transformations = {}\n",
    "\n",
    "# 1. Dados originais (binários)\n",
    "transformations['Original'] = X.values\n",
    "\n",
    "# 2. PCA para capturar variância principal\n",
    "pca = PCA(n_components=0.95, random_state=42)  # Manter 95% da variância\n",
    "X_pca = pca.fit_transform(X)\n",
    "transformations['PCA'] = X_pca\n",
    "print(f\"✅ PCA: {X_pca.shape[1]} componentes mantendo 95% da variância\")\n",
    "\n",
    "# 3. Criação de perfis agregados MELHORADOS\n",
    "print(\"📊 Criando perfis agregados avançados...\")\n",
    "\n",
    "# Identificar grupos baseados no ENANI\n",
    "X_perfis = pd.DataFrame()\n",
    "\n",
    "# IDADE 0-6 MESES (aleitamento materno predominante)\n",
    "if 'e01_leite_peito' in X.columns:\n",
    "    X_perfis['amamentacao'] = X['e01_leite_peito']\n",
    "    \n",
    "# SUBSTITUTOS DO LEITE MATERNO\n",
    "leites_substitutos = ['e06_leite_vaca_po', 'e07_leite_vaca_liquido', \n",
    "                     'e10_formula_infantil']\n",
    "leites_existentes = [col for col in leites_substitutos if col in X.columns]\n",
    "if leites_existentes:\n",
    "    X_perfis['leites_substitutos'] = X[leites_existentes].max(axis=1)\n",
    "\n",
    "# INDICADOR DE INTRODUÇÃO ALIMENTAR\n",
    "if 'e214a_nao_comeu' in X.columns:\n",
    "    X_perfis['ja_come_solidos'] = 1 - X['e214a_nao_comeu']\n",
    "    X_perfis['alimentacao_exclusiva_leite'] = X['e214a_nao_comeu']\n",
    "\n",
    "# ALIMENTOS BÁSICOS BRASILEIROS\n",
    "basicos = ['e21_arroz', 'e26_feijao']\n",
    "basicos_exist = [col for col in basicos if col in X.columns]\n",
    "if basicos_exist:\n",
    "    X_perfis['alimentos_basicos'] = X[basicos_exist].mean(axis=1)\n",
    "\n",
    "# PROTEÍNAS\n",
    "proteinas = ['e27_carne', 'e29_ovo']\n",
    "proteinas_exist = [col for col in proteinas if col in X.columns]\n",
    "if proteinas_exist:\n",
    "    X_perfis['consumo_proteinas'] = X[proteinas_exist].max(axis=1)\n",
    "\n",
    "# FRUTAS E VEGETAIS\n",
    "frutas_veg = ['e12_fruta_inteira', 'e22_legumes', 'e23_cenoura', 'e24_couve', 'e25_verduras']\n",
    "fv_exist = [col for col in frutas_veg if col in X.columns]\n",
    "if fv_exist:\n",
    "    X_perfis['frutas_vegetais'] = X[fv_exist].mean(axis=1)\n",
    "\n",
    "# ULTRAPROCESSADOS (crítico no ENANI)\n",
    "ultraproc = ['e30_hamburger', 'e31_salgadinhos', 'e32_suco_industrializado', \n",
    "             'e33_refrigerante', 'e35_biscoito', 'e36_bala']\n",
    "ultra_exist = [col for col in ultraproc if col in X.columns]\n",
    "if ultra_exist:\n",
    "    X_perfis['ultraprocessados'] = X[ultra_exist].sum(axis=1) / len(ultra_exist)\n",
    "\n",
    "# PREPARAÇÕES ESPECIAIS PARA BEBÊS\n",
    "if 'e19_mingau' in X.columns:\n",
    "    X_perfis['mingau'] = X['e19_mingau']\n",
    "\n",
    "# TEXTURA DOS ALIMENTOS (indicador de desenvolvimento)\n",
    "texturas = ['e181_pedacos', 'e182_amassada', 'e183_peneira', 'e184_liquidificada']\n",
    "texturas_exist = [col for col in texturas if col in X.columns]\n",
    "if texturas_exist:\n",
    "    # Pedaços indica desenvolvimento mais avançado\n",
    "    if 'e181_pedacos' in X.columns:\n",
    "        X_perfis['alimentacao_solida_avancada'] = X['e181_pedacos']\n",
    "\n",
    "# INDICADORES DE RISCO\n",
    "if 'e39_mamadeira' in X.columns:\n",
    "    X_perfis['uso_mamadeira'] = X['e39_mamadeira']\n",
    "if 'e40_adocado' in X.columns:\n",
    "    X_perfis['alimentos_adocados'] = X['e40_adocado']\n",
    "\n",
    "# DIVERSIDADE ALIMENTAR (importante no ENANI)\n",
    "X_perfis['diversidade_total'] = (X > 0).sum(axis=1) / X.shape[1]\n",
    "X_perfis['n_grupos_alimentares'] = (X_perfis > 0).sum(axis=1)\n",
    "\n",
    "# PADRÕES ESPECÍFICOS POR IDADE\n",
    "# Bebês muito pequenos: só leite\n",
    "X_perfis['padrao_0_6m'] = ((X_perfis.get('amamentacao', 0) > 0) | \n",
    "                           (X_perfis.get('leites_substitutos', 0) > 0)) & \\\n",
    "                          (X_perfis.get('ja_come_solidos', 1) == 0)\n",
    "\n",
    "# Transição alimentar: leite + alguns sólidos\n",
    "X_perfis['padrao_transicao'] = (X_perfis.get('ja_come_solidos', 0) > 0) & \\\n",
    "                                (X_perfis.get('diversidade_total', 0) < 0.3)\n",
    "\n",
    "# Alimentação estabelecida: diversa\n",
    "X_perfis['padrao_estabelecido'] = X_perfis.get('diversidade_total', 0) > 0.5\n",
    "\n",
    "transformations['Perfis'] = X_perfis.values\n",
    "print(f\"✅ Perfis criados: {X_perfis.shape[1]} variáveis agregadas\")\n",
    "\n",
    "# 4. Combinação de múltiplas representações\n",
    "X_combined = np.hstack([\n",
    "    StandardScaler().fit_transform(X.values),\n",
    "    StandardScaler().fit_transform(X_perfis.values)\n",
    "])\n",
    "transformations['Combinado'] = X_combined\n",
    "print(f\"✅ Representação combinada: {X_combined.shape[1]} features\")\n",
    "\n",
    "# ====================================================================\n",
    "# 3. TESTE DE MÚLTIPLOS ALGORITMOS\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n🔬 3. TESTANDO MÚLTIPLOS ALGORITMOS DE CLUSTERING\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Para cada transformação, testar vários algoritmos\n",
    "results_all = {}\n",
    "K_range = range(4, 15)  # FORÇAR mínimo de 4 clusters\n",
    "\n",
    "for trans_name, X_trans in transformations.items():\n",
    "    print(f\"\\n📊 Testando com {trans_name}...\")\n",
    "    \n",
    "    # Normalizar sempre\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_trans)\n",
    "    \n",
    "    results_trans = {}\n",
    "    \n",
    "    # 1. K-Means com múltiplas inicializações\n",
    "    print(\"  🔸 K-Means com inicialização k-means++...\")\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=50, max_iter=1000, \n",
    "                       init='k-means++', random_state=42)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        # Validar clusters\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(unique_labels) < k:\n",
    "            continue\n",
    "            \n",
    "        sil = silhouette_score(X_scaled, labels)\n",
    "        cal = calinski_harabasz_score(X_scaled, labels)\n",
    "        \n",
    "        # Verificar balanceamento\n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        min_size = min(counts)\n",
    "        max_size = max(counts)\n",
    "        balance = min_size / max_size\n",
    "        \n",
    "        # Aceitar clusters menores para ter mais grupos\n",
    "        if min_size >= 50:  # Reduzido de 100 para 50\n",
    "            score = sil * 0.4 + (k/10) * 0.3 + balance * 0.3  # Favorece mais clusters\n",
    "            results_trans[f'kmeans_{k}'] = {\n",
    "                'labels': labels,\n",
    "                'silhouette': sil,\n",
    "                'calinski': cal,\n",
    "                'n_clusters': k,\n",
    "                'min_size': min_size,\n",
    "                'balance': balance,\n",
    "                'score': score,\n",
    "                'algorithm': 'kmeans'\n",
    "            }\n",
    "    \n",
    "    # 2. Mini-Batch K-Means (mais robusto para dados grandes)\n",
    "    print(\"  🔸 Mini-Batch K-Means...\")\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    for k in range(5, 12):\n",
    "        mbk = MiniBatchKMeans(n_clusters=k, batch_size=1000, n_init=30, random_state=42)\n",
    "        labels = mbk.fit_predict(X_scaled)\n",
    "        \n",
    "        if len(np.unique(labels)) == k:\n",
    "            sil = silhouette_score(X_scaled, labels, sample_size=5000)\n",
    "            _, counts = np.unique(labels, return_counts=True)\n",
    "            min_size = min(counts)\n",
    "            \n",
    "            if min_size >= 50:\n",
    "                results_trans[f'minibatch_{k}'] = {\n",
    "                    'labels': labels,\n",
    "                    'silhouette': sil,\n",
    "                    'n_clusters': k,\n",
    "                    'min_size': min_size,\n",
    "                    'algorithm': 'minibatch'\n",
    "                }\n",
    "    \n",
    "    # 3. Hierarchical com Ward (melhor para grupos compactos)\n",
    "    print(\"  🔸 Hierarchical Ward...\")\n",
    "    for k in [5, 6, 7, 8]:\n",
    "        try:\n",
    "            hc = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "            labels = hc.fit_predict(X_scaled)\n",
    "            \n",
    "            sil = silhouette_score(X_scaled, labels, sample_size=5000)\n",
    "            _, counts = np.unique(labels, return_counts=True)\n",
    "            min_size = min(counts)\n",
    "            \n",
    "            if min_size >= 100:\n",
    "                results_trans[f'hierarchical_ward_{k}'] = {\n",
    "                    'labels': labels,\n",
    "                    'silhouette': sil,\n",
    "                    'n_clusters': k,\n",
    "                    'min_size': min_size,\n",
    "                    'algorithm': 'hierarchical_ward'\n",
    "                }\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # 4. DBSCAN adaptativo (encontra clusters naturalmente)\n",
    "    print(\"  🔸 DBSCAN adaptativo...\")\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    # Encontrar eps ótimo\n",
    "    nn = NearestNeighbors(n_neighbors=50)\n",
    "    nn.fit(X_scaled)\n",
    "    distances, _ = nn.kneighbors(X_scaled)\n",
    "    distances = np.sort(distances[:, -1])\n",
    "    eps_candidates = np.percentile(distances, [85, 90, 95])\n",
    "    \n",
    "    for eps in eps_candidates:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=30)\n",
    "        labels = dbscan.fit_predict(X_scaled)\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        if 4 <= n_clusters <= 12:\n",
    "            # Reassign outliers to nearest cluster\n",
    "            if -1 in labels:\n",
    "                from sklearn.neighbors import KNeighborsClassifier\n",
    "                mask = labels != -1\n",
    "                if mask.sum() > 0:\n",
    "                    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "                    knn.fit(X_scaled[mask], labels[mask])\n",
    "                    labels[~mask] = knn.predict(X_scaled[~mask])\n",
    "            \n",
    "            _, counts = np.unique(labels, return_counts=True)\n",
    "            min_size = min(counts)\n",
    "            \n",
    "            if min_size >= 100 and len(counts) >= 4:\n",
    "                sil = silhouette_score(X_scaled, labels, sample_size=5000)\n",
    "                results_trans[f'dbscan_{eps:.2f}'] = {\n",
    "                    'labels': labels,\n",
    "                    'silhouette': sil,\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'min_size': min_size,\n",
    "                    'algorithm': 'dbscan'\n",
    "                }\n",
    "    \n",
    "    results_all[trans_name] = results_trans\n",
    "\n",
    "# ====================================================================\n",
    "# 4. SELEÇÃO DO MELHOR RESULTADO\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n🏆 4. SELEÇÃO DO MELHOR CLUSTERING\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Encontrar melhor combinação\n",
    "best_score = -1\n",
    "best_config = None\n",
    "all_configs = []\n",
    "\n",
    "for trans_name, results in results_all.items():\n",
    "    for config_name, config in results.items():\n",
    "        n_clusters = config['n_clusters']\n",
    "        silhouette = config.get('silhouette', 0)\n",
    "        min_size = config['min_size']\n",
    "        \n",
    "        # Score que FORTEMENTE favorece mais clusters\n",
    "        # Penaliza fortemente soluções com poucos clusters\n",
    "        if n_clusters < 4:\n",
    "            cluster_bonus = 0.1\n",
    "        elif n_clusters < 6:\n",
    "            cluster_bonus = 0.5\n",
    "        else:\n",
    "            cluster_bonus = 1.0 + (n_clusters - 6) * 0.1\n",
    "        \n",
    "        # Aceitar silhouette mais baixo em troca de mais clusters\n",
    "        score = (silhouette + 0.3) * cluster_bonus * (min_size / 200)\n",
    "        \n",
    "        config['final_score'] = score\n",
    "        config['trans_name'] = trans_name\n",
    "        config['config_name'] = config_name\n",
    "        all_configs.append(config)\n",
    "        \n",
    "        if score > best_score and n_clusters >= 5:\n",
    "            best_score = score\n",
    "            best_config = (trans_name, config_name, config)\n",
    "\n",
    "# Se não encontrou solução com 5+ clusters, forçar\n",
    "if best_config is None or best_config[2]['n_clusters'] < 5:\n",
    "    print(\"⚠️ Forçando solução com mais clusters...\")\n",
    "    \n",
    "    # Tentar K-means com k=6 ou k=7 na representação combinada\n",
    "    X_scaled = StandardScaler().fit_transform(transformations['Combinado'])\n",
    "    \n",
    "    for k in [7, 6, 8, 5]:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=100, max_iter=1000, \n",
    "                       init='k-means++', random_state=42)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        min_size = min(counts)\n",
    "        \n",
    "        if min_size >= 50:  # Aceitar clusters menores\n",
    "            sil = silhouette_score(X_scaled, labels, sample_size=5000)\n",
    "            best_config = ('Combinado_Forcado', f'kmeans_{k}', {\n",
    "                'labels': labels,\n",
    "                'silhouette': sil,\n",
    "                'n_clusters': k,\n",
    "                'min_size': min_size,\n",
    "                'algorithm': 'kmeans_forced'\n",
    "            })\n",
    "            break\n",
    "\n",
    "# Mostrar top 5 configurações\n",
    "print(\"\\n📊 Top 5 configurações encontradas:\")\n",
    "all_configs.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "for i, cfg in enumerate(all_configs[:5]):\n",
    "    print(f\"{i+1}. {cfg['trans_name']} - {cfg['algorithm']} \"\n",
    "          f\"(K={cfg['n_clusters']}, Sil={cfg.get('silhouette', 0):.3f}, \"\n",
    "          f\"MinSize={cfg['min_size']}, Score={cfg['final_score']:.3f})\")\n",
    "\n",
    "if best_config:\n",
    "    trans_name, config_name, config = best_config\n",
    "    best_labels = config['labels']\n",
    "    n_clusters = config['n_clusters']\n",
    "    \n",
    "    print(f\"\\n✅ Configuração selecionada:\")\n",
    "    print(f\"   Transformação: {trans_name}\")\n",
    "    print(f\"   Algoritmo: {config['algorithm']}\")\n",
    "    print(f\"   Número de clusters: {n_clusters}\")\n",
    "    print(f\"   Silhouette: {config.get('silhouette', 'N/A')}\")\n",
    "    print(f\"   Menor cluster: {config['min_size']:,} crianças\")\n",
    "else:\n",
    "    print(\"\\n❌ Nenhuma configuração adequada encontrada!\")\n",
    "    print(\"Usando fallback com 6 clusters...\")\n",
    "    X_scaled = StandardScaler().fit_transform(X_perfis)\n",
    "    kmeans = KMeans(n_clusters=6, n_init=50, random_state=42)\n",
    "    best_labels = kmeans.fit_predict(X_scaled)\n",
    "    n_clusters = 6\n",
    "\n",
    "if best_config:\n",
    "    trans_name, config_name, config = best_config\n",
    "    best_labels = config['labels']\n",
    "    n_clusters = config['n_clusters']\n",
    "    \n",
    "    print(f\"✅ Melhor configuração encontrada:\")\n",
    "    print(f\"   Transformação: {trans_name}\")\n",
    "    print(f\"   Algoritmo: {config['algorithm']}\")\n",
    "    print(f\"   Número de clusters: {n_clusters}\")\n",
    "    print(f\"   Silhouette: {config['silhouette']:.3f}\")\n",
    "    print(f\"   Menor cluster: {config['min_size']:,} crianças\")\n",
    "else:\n",
    "    # Fallback: forçar clustering com mais grupos\n",
    "    print(\"⚠️ Forçando clustering com 6 grupos...\")\n",
    "    X_scaled = StandardScaler().fit_transform(X_perfis)\n",
    "    kmeans = KMeans(n_clusters=6, n_init=50, random_state=42)\n",
    "    best_labels = kmeans.fit_predict(X_scaled)\n",
    "    n_clusters = 6\n",
    "\n",
    "# ====================================================================\n",
    "# 5. ANÁLISE DETALHADA DOS CLUSTERS\n",
    "# ====================================================================\n",
    "\n",
    "print(f\"\\n📊 5. ANÁLISE DOS {n_clusters} CLUSTERS ENCONTRADOS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Adicionar clusters ao dataset\n",
    "df_final = df_clean.copy()\n",
    "df_final['cluster'] = best_labels\n",
    "\n",
    "# Estatísticas por cluster\n",
    "for cluster in sorted(np.unique(best_labels)):\n",
    "    cluster_data = df_final[df_final['cluster'] == cluster]\n",
    "    n_criancas = len(cluster_data)\n",
    "    pct = n_criancas / len(df_final) * 100\n",
    "    \n",
    "    print(f\"\\n🏷️ CLUSTER {cluster}\")\n",
    "    print(f\"   Tamanho: {n_criancas:,} crianças ({pct:.1f}%)\")\n",
    "    \n",
    "    # Calcular médias do cluster\n",
    "    cluster_means = cluster_data[X.columns].mean()\n",
    "    overall_means = df_final[X.columns].mean()\n",
    "    \n",
    "    # Top 10 alimentos\n",
    "    top_10 = cluster_means.nlargest(10)\n",
    "    print(\"\\n   📈 Top 10 alimentos mais consumidos:\")\n",
    "    for food, value in top_10.items():\n",
    "        diff = value - overall_means[food]\n",
    "        diff_str = f\"({diff:+.1%} vs média)\" if abs(diff) > 0.1 else \"\"\n",
    "        print(f\"      • {food}: {value:.1%} {diff_str}\")\n",
    "    \n",
    "    # Características distintivas (>20% diferença)\n",
    "    diffs = cluster_means - overall_means\n",
    "    distintivos = diffs[abs(diffs) > 0.2].sort_values(ascending=False)\n",
    "    \n",
    "    if len(distintivos) > 0:\n",
    "        print(\"\\n   ⭐ Características distintivas:\")\n",
    "        for food, diff in distintivos.items():\n",
    "            if diff > 0:\n",
    "                print(f\"      ↑ {food}: {diff:+.1%} acima da média\")\n",
    "            else:\n",
    "                print(f\"      ↓ {food}: {diff:.1%} abaixo da média\")\n",
    "    \n",
    "    # Perfil resumido\n",
    "    if 'e01_leite_peito' in cluster_means.index:\n",
    "        amamentacao = cluster_means['e01_leite_peito']\n",
    "        solidos = 1 - cluster_means.get('e214a_nao_comeu', 0)\n",
    "        ultraproc = cluster_means[['e30_hamburger', 'e31_salgadinhos', \n",
    "                                  'e32_suco_industrializado', 'e33_refrigerante', \n",
    "                                  'e35_biscoito', 'e36_bala']].mean()\n",
    "        \n",
    "        print(f\"\\n   📋 Perfil resumido:\")\n",
    "        print(f\"      • Amamentação: {amamentacao:.1%}\")\n",
    "        print(f\"      • Alimentos sólidos: {solidos:.1%}\")\n",
    "        print(f\"      • Ultraprocessados: {ultraproc:.1%}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 6. VISUALIZAÇÃO E SALVAMENTO\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n💾 6. SALVANDO RESULTADOS E VISUALIZAÇÕES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "import os\n",
    "output_path = '/Users/marcelosilva/Desktop/clustering(0-4)/4-Clustering/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Salvar resultados principais\n",
    "results_df = pd.DataFrame({\n",
    "    'id_anon': ids,\n",
    "    'cluster': best_labels\n",
    "})\n",
    "results_df.to_csv(output_path + 'clustering_multiplos_grupos.csv', index=False)\n",
    "print(f\"✅ Resultado principal salvo: {n_clusters} clusters\")\n",
    "\n",
    "# Dataset completo\n",
    "df_final.to_csv(output_path + 'dataset_completo_clusters.csv', index=False)\n",
    "\n",
    "# Características por cluster\n",
    "cluster_profiles = df_final.groupby('cluster')[X.columns].mean()\n",
    "cluster_profiles.to_csv(output_path + 'perfis_clusters.csv')\n",
    "\n",
    "# Criar visualização\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Distribuição dos clusters\n",
    "plt.subplot(2, 2, 1)\n",
    "cluster_counts = df_final['cluster'].value_counts().sort_index()\n",
    "plt.bar(cluster_counts.index, cluster_counts.values)\n",
    "plt.title(f'Distribuição dos {n_clusters} Clusters')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Número de Crianças')\n",
    "for i, v in enumerate(cluster_counts.values):\n",
    "    plt.text(i, v + 50, f'{v:,}\\n({v/len(df_final)*100:.1f}%)', \n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# Subplot 2: PCA visualization\n",
    "plt.subplot(2, 2, 2)\n",
    "pca_viz = PCA(n_components=2, random_state=42)\n",
    "X_pca_viz = pca_viz.fit_transform(X)\n",
    "scatter = plt.scatter(X_pca_viz[:, 0], X_pca_viz[:, 1], \n",
    "                     c=best_labels, cmap='tab10', alpha=0.6, s=1)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('Visualização PCA dos Clusters')\n",
    "plt.xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.1%} var)')\n",
    "plt.ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.1%} var)')\n",
    "\n",
    "# Subplot 3: Heatmap dos perfis\n",
    "plt.subplot(2, 1, 2)\n",
    "# Selecionar variáveis mais importantes\n",
    "important_vars = []\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_means = df_final[df_final['cluster'] == cluster][X.columns].mean()\n",
    "    overall_means = df_final[X.columns].mean()\n",
    "    diffs = abs(cluster_means - overall_means)\n",
    "    important_vars.extend(diffs.nlargest(5).index.tolist())\n",
    "important_vars = list(set(important_vars))[:20]  # Top 20 variáveis distintivas\n",
    "\n",
    "heatmap_data = cluster_profiles[important_vars].T\n",
    "sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Proporção de Consumo'})\n",
    "plt.title('Perfil de Consumo por Cluster (Top 20 Alimentos Distintivos)')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Alimento')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_path + 'visualizacao_clusters.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✅ Visualizações salvas\")\n",
    "\n",
    "# Relatório resumido\n",
    "with open(output_path + 'relatorio_clusters.txt', 'w') as f:\n",
    "    f.write(f\"RELATÓRIO DE CLUSTERING - PADRÕES ALIMENTARES INFANTIS\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Total de crianças: {len(df_final):,}\\n\")\n",
    "    f.write(f\"Número de clusters: {n_clusters}\\n\\n\")\n",
    "    \n",
    "    for cluster in sorted(np.unique(best_labels)):\n",
    "        n = len(df_final[df_final['cluster'] == cluster])\n",
    "        f.write(f\"\\nCLUSTER {cluster}: {n:,} crianças ({n/len(df_final)*100:.1f}%)\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        # Características principais\n",
    "        cluster_means = df_final[df_final['cluster'] == cluster][X.columns].mean()\n",
    "        top_5 = cluster_means.nlargest(5)\n",
    "        \n",
    "        f.write(\"Principais alimentos:\\n\")\n",
    "        for food, value in top_5.items():\n",
    "            f.write(f\"  - {food}: {value:.1%}\\n\")\n",
    "\n",
    "print(f\"\\n🎉 CLUSTERING CONCLUÍDO COM SUCESSO!\")\n",
    "print(f\"📊 {n_clusters} clusters identificados\")\n",
    "print(f\"📁 Resultados salvos em: {output_path}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
