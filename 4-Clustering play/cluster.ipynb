{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccdcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# CLUSTERING APRIMORADO - CAPTURANDO M√öLTIPLOS PADR√ïES ALIMENTARES\n",
    "# Melhorias: Tratamento de dados bin√°rios, m√∫ltiplos m√©todos, valida√ß√£o robusta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from kneed import KneeLocator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üçé CLUSTERING APRIMORADO - AN√ÅLISE DE PADR√ïES ALIMENTARES INFANTIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ Objetivo: Identificar m√∫ltiplos grupos com padr√µes alimentares distintos\")\n",
    "print(\"üìä Melhorias: Tratamento especial para dados bin√°rios, m√∫ltiplos algoritmos\")\n",
    "print()\n",
    "\n",
    "# ====================================================================\n",
    "# 1. CARREGAR E PREPARAR DADOS\n",
    "# ====================================================================\n",
    "\n",
    "print(\"üìä 1. CARREGANDO E PREPARANDO DATASET\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "df = pd.read_csv('/Users/marcelosilva/Desktop/clustering(0-4)/3-E-Aval/DSFinal.csv')\n",
    "\n",
    "print(f\"‚úÖ Dataset original: {df.shape[0]:,} crian√ßas √ó {df.shape[1]} vari√°veis\")\n",
    "\n",
    "# Remover vari√°veis de frequ√™ncia e preparar dados\n",
    "frequency_vars = ['e13_fruta_vezes', 'e17_sal_vezes']\n",
    "# Verificar quais colunas realmente existem\n",
    "cols_to_remove = [col for col in frequency_vars if col in df.columns]\n",
    "if cols_to_remove:\n",
    "    df_filtered = df.drop(columns=cols_to_remove)\n",
    "else:\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "# Verificar e remover colunas string se existirem\n",
    "string_cols = ['e18_oferecida', 'e21a_pao']\n",
    "existing_string_cols = [col for col in string_cols if col in df_filtered.columns]\n",
    "if existing_string_cols:\n",
    "    df_clean = df_filtered.drop(columns=existing_string_cols)\n",
    "else:\n",
    "    df_clean = df_filtered.copy()\n",
    "\n",
    "X = df_clean.drop('id_anon', axis=1)\n",
    "ids = df_clean['id_anon']\n",
    "\n",
    "print(f\"‚úÖ Dataset limpo: {X.shape[0]:,} crian√ßas √ó {X.shape[1]} vari√°veis\")\n",
    "\n",
    "# An√°lise da natureza dos dados\n",
    "binary_count = sum((X[col].nunique() == 2) for col in X.columns)\n",
    "print(f\"üìä Vari√°veis bin√°rias: {binary_count}/{X.shape[1]} ({binary_count/X.shape[1]*100:.1f}%)\")\n",
    "\n",
    "# Verificar distribui√ß√£o dos dados\n",
    "print(\"\\nüîç An√°lise da distribui√ß√£o dos dados:\")\n",
    "prevalence = X.mean()\n",
    "print(f\"   Alimentos com preval√™ncia > 80%: {sum(prevalence > 0.8)}\")\n",
    "print(f\"   Alimentos com preval√™ncia < 20%: {sum(prevalence < 0.2)}\")\n",
    "print(f\"   Alimentos com preval√™ncia 20-80%: {sum((prevalence >= 0.2) & (prevalence <= 0.8))}\")\n",
    "\n",
    "# Listar colunas dispon√≠veis para debugging\n",
    "print(f\"\\nüìã Primeiras 10 vari√°veis dispon√≠veis: {list(X.columns[:10])}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. TRANSFORMA√á√ÉO INTELIGENTE DOS DADOS\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n‚öôÔ∏è 2. TRANSFORMA√á√ÉO INTELIGENTE DOS DADOS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar diferentes representa√ß√µes dos dados\n",
    "transformations = {}\n",
    "\n",
    "# 1. Dados originais (bin√°rios)\n",
    "transformations['Original'] = X.values\n",
    "\n",
    "# 2. PCA para capturar vari√¢ncia principal\n",
    "pca = PCA(n_components=0.95, random_state=42)  # Manter 95% da vari√¢ncia\n",
    "X_pca = pca.fit_transform(X)\n",
    "transformations['PCA'] = X_pca\n",
    "print(f\"‚úÖ PCA: {X_pca.shape[1]} componentes mantendo 95% da vari√¢ncia\")\n",
    "\n",
    "# 3. Cria√ß√£o de perfis agregados MELHORADOS\n",
    "print(\"üìä Criando perfis agregados avan√ßados...\")\n",
    "\n",
    "# Identificar grupos baseados no ENANI\n",
    "X_perfis = pd.DataFrame()\n",
    "\n",
    "# IDADE 0-6 MESES (aleitamento materno predominante)\n",
    "if 'e01_leite_peito' in X.columns:\n",
    "    X_perfis['amamentacao'] = X['e01_leite_peito']\n",
    "    \n",
    "# SUBSTITUTOS DO LEITE MATERNO\n",
    "leites_substitutos = ['e06_leite_vaca_po', 'e07_leite_vaca_liquido', \n",
    "                     'e10_formula_infantil']\n",
    "leites_existentes = [col for col in leites_substitutos if col in X.columns]\n",
    "if leites_existentes:\n",
    "    X_perfis['leites_substitutos'] = X[leites_existentes].max(axis=1)\n",
    "\n",
    "# INDICADOR DE INTRODU√á√ÉO ALIMENTAR\n",
    "if 'e214a_nao_comeu' in X.columns:\n",
    "    X_perfis['ja_come_solidos'] = 1 - X['e214a_nao_comeu']\n",
    "    X_perfis['alimentacao_exclusiva_leite'] = X['e214a_nao_comeu']\n",
    "\n",
    "# ALIMENTOS B√ÅSICOS BRASILEIROS\n",
    "basicos = ['e21_arroz', 'e26_feijao']\n",
    "basicos_exist = [col for col in basicos if col in X.columns]\n",
    "if basicos_exist:\n",
    "    X_perfis['alimentos_basicos'] = X[basicos_exist].mean(axis=1)\n",
    "\n",
    "# PROTE√çNAS\n",
    "proteinas = ['e27_carne', 'e29_ovo']\n",
    "proteinas_exist = [col for col in proteinas if col in X.columns]\n",
    "if proteinas_exist:\n",
    "    X_perfis['consumo_proteinas'] = X[proteinas_exist].max(axis=1)\n",
    "\n",
    "# FRUTAS E VEGETAIS\n",
    "frutas_veg = ['e12_fruta_inteira', 'e22_legumes', 'e23_cenoura', 'e24_couve', 'e25_verduras']\n",
    "fv_exist = [col for col in frutas_veg if col in X.columns]\n",
    "if fv_exist:\n",
    "    X_perfis['frutas_vegetais'] = X[fv_exist].mean(axis=1)\n",
    "\n",
    "# ULTRAPROCESSADOS (cr√≠tico no ENANI)\n",
    "ultraproc = ['e30_hamburger', 'e31_salgadinhos', 'e32_suco_industrializado', \n",
    "             'e33_refrigerante', 'e35_biscoito', 'e36_bala']\n",
    "ultra_exist = [col for col in ultraproc if col in X.columns]\n",
    "if ultra_exist:\n",
    "    X_perfis['ultraprocessados'] = X[ultra_exist].sum(axis=1) / len(ultra_exist)\n",
    "\n",
    "# PREPARA√á√ïES ESPECIAIS PARA BEB√äS\n",
    "if 'e19_mingau' in X.columns:\n",
    "    X_perfis['mingau'] = X['e19_mingau']\n",
    "\n",
    "# TEXTURA DOS ALIMENTOS (indicador de desenvolvimento)\n",
    "texturas = ['e181_pedacos', 'e182_amassada', 'e183_peneira', 'e184_liquidificada']\n",
    "texturas_exist = [col for col in texturas if col in X.columns]\n",
    "if texturas_exist:\n",
    "    # Peda√ßos indica desenvolvimento mais avan√ßado\n",
    "    if 'e181_pedacos' in X.columns:\n",
    "        X_perfis['alimentacao_solida_avancada'] = X['e181_pedacos']\n",
    "\n",
    "# INDICADORES DE RISCO\n",
    "if 'e39_mamadeira' in X.columns:\n",
    "    X_perfis['uso_mamadeira'] = X['e39_mamadeira']\n",
    "if 'e40_adocado' in X.columns:\n",
    "    X_perfis['alimentos_adocados'] = X['e40_adocado']\n",
    "\n",
    "# DIVERSIDADE ALIMENTAR (importante no ENANI)\n",
    "X_perfis['diversidade_total'] = (X > 0).sum(axis=1) / X.shape[1]\n",
    "X_perfis['n_grupos_alimentares'] = (X_perfis > 0).sum(axis=1)\n",
    "\n",
    "# PADR√ïES ESPEC√çFICOS POR IDADE\n",
    "# Beb√™s muito pequenos: s√≥ leite\n",
    "X_perfis['padrao_0_6m'] = ((X_perfis.get('amamentacao', 0) > 0) | \n",
    "                           (X_perfis.get('leites_substitutos', 0) > 0)) & \\\n",
    "                          (X_perfis.get('ja_come_solidos', 1) == 0)\n",
    "\n",
    "# Transi√ß√£o alimentar: leite + alguns s√≥lidos\n",
    "X_perfis['padrao_transicao'] = (X_perfis.get('ja_come_solidos', 0) > 0) & \\\n",
    "                                (X_perfis.get('diversidade_total', 0) < 0.3)\n",
    "\n",
    "# Alimenta√ß√£o estabelecida: diversa\n",
    "X_perfis['padrao_estabelecido'] = X_perfis.get('diversidade_total', 0) > 0.5\n",
    "\n",
    "transformations['Perfis'] = X_perfis.values\n",
    "print(f\"‚úÖ Perfis criados: {X_perfis.shape[1]} vari√°veis agregadas\")\n",
    "\n",
    "# 4. Combina√ß√£o de m√∫ltiplas representa√ß√µes\n",
    "X_combined = np.hstack([\n",
    "    StandardScaler().fit_transform(X.values),\n",
    "    StandardScaler().fit_transform(X_perfis.values)\n",
    "])\n",
    "transformations['Combinado'] = X_combined\n",
    "print(f\"‚úÖ Representa√ß√£o combinada: {X_combined.shape[1]} features\")\n",
    "\n",
    "# ====================================================================\n",
    "# 3. TESTE DE M√öLTIPLOS ALGORITMOS\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\nüî¨ 3. TESTANDO M√öLTIPLOS ALGORITMOS DE CLUSTERING\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Para cada transforma√ß√£o, testar v√°rios algoritmos\n",
    "results_all = {}\n",
    "K_range = range(4, 15)  # FOR√áAR m√≠nimo de 4 clusters\n",
    "\n",
    "for trans_name, X_trans in transformations.items():\n",
    "    print(f\"\\nüìä Testando com {trans_name}...\")\n",
    "    \n",
    "    # Normalizar sempre\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_trans)\n",
    "    \n",
    "    results_trans = {}\n",
    "    \n",
    "    # 1. K-Means com m√∫ltiplas inicializa√ß√µes\n",
    "    print(\"  üî∏ K-Means com inicializa√ß√£o k-means++...\")\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=50, max_iter=1000, \n",
    "                       init='k-means++', random_state=42)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        # Validar clusters\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(unique_labels) < k:\n",
    "            continue\n",
    "            \n",
    "        sil = silhouette_score(X_scaled, labels)\n",
    "        cal = calinski_harabasz_score(X_scaled, labels)\n",
    "        \n",
    "        # Verificar balanceamento\n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        min_size = min(counts)\n",
    "        max_size = max(counts)\n",
    "        balance = min_size / max_size\n",
    "        \n",
    "        # Aceitar clusters menores para ter mais grupos\n",
    "        if min_size >= 50:  # Reduzido de 100 para 50\n",
    "            score = sil * 0.4 + (k/10) * 0.3 + balance * 0.3  # Favorece mais clusters\n",
    "            results_trans[f'kmeans_{k}'] = {\n",
    "                'labels': labels,\n",
    "                'silhouette': sil,\n",
    "                'calinski': cal,\n",
    "                'n_clusters': k,\n",
    "                'min_size': min_size,\n",
    "                'balance': balance,\n",
    "                'score': score,\n",
    "                'algorithm': 'kmeans'\n",
    "            }\n",
    "    \n",
    "    # 2. Mini-Batch K-Means (mais robusto para dados grandes)\n",
    "    print(\"  üî∏ Mini-Batch K-Means...\")\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    for k in range(5, 12):\n",
    "        mbk = MiniBatchKMeans(n_clusters=k, batch_size=1000, n_init=30, random_state=42)\n",
    "        labels = mbk.fit_predict(X_scaled)\n",
    "        \n",
    "        if len(np.unique(labels)) == k:\n",
    "            sil = silhouette_score(X_scaled, labels, sample_size=5000)\n",
    "            _, counts = np.unique(labels, return_counts=True)\n",
    "            min_size = min(counts)\n",
    "            \n",
    "            if min_size >= 50:\n",
    "                results_trans[f'minibatch_{k}'] = {\n",
    "                    'labels': labels,\n",
    "                    'silhouette': sil,\n",
    "                    'n_clusters': k,\n",
    "                    'min_size': min_size,\n",
    "                    'algorithm': 'minibatch'\n",
    "                }\n",
    "    \n",
    "    # 3. Hierarchical com Ward (melhor para grupos compactos)\n",
    "    print(\"  üî∏ Hierarchical Ward...\")\n",
    "    for k in [5, 6, 7, 8]:\n",
    "        try:\n",
    "            hc = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "            labels = hc.fit_predict(X_scaled)\n",
    "            \n",
    "            sil = silhouette_score(X_scaled, labels, sample_size=5000)\n",
    "            _, counts = np.unique(labels, return_counts=True)\n",
    "            min_size = min(counts)\n",
    "            \n",
    "            if min_size >= 100:\n",
    "                results_trans[f'hierarchical_ward_{k}'] = {\n",
    "                    'labels': labels,\n",
    "                    'silhouette': sil,\n",
    "                    'n_clusters': k,\n",
    "                    'min_size': min_size,\n",
    "                    'algorithm': 'hierarchical_ward'\n",
    "                }\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # 4. DBSCAN adaptativo (encontra clusters naturalmente)\n",
    "    print(\"  üî∏ DBSCAN adaptativo...\")\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    # Encontrar eps √≥timo\n",
    "    nn = NearestNeighbors(n_neighbors=50)\n",
    "    nn.fit(X_scaled)\n",
    "    distances, _ = nn.kneighbors(X_scaled)\n",
    "    distances = np.sort(distances[:, -1])\n",
    "    eps_candidates = np.percentile(distances, [85, 90, 95])\n",
    "    \n",
    "    for eps in eps_candidates:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=30)\n",
    "        labels = dbscan.fit_predict(X_scaled)\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        if 4 <= n_clusters <= 12:\n",
    "            # Reassign outliers to nearest cluster\n",
    "            if -1 in labels:\n",
    "                from sklearn.neighbors import KNeighborsClassifier\n",
    "                mask = labels != -1\n",
    "                if mask.sum() > 0:\n",
    "                    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "                    knn.fit(X_scaled[mask], labels[mask])\n",
    "                    labels[~mask] = knn.predict(X_scaled[~mask])\n",
    "            \n",
    "            _, counts = np.unique(labels, return_counts=True)\n",
    "            min_size = min(counts)\n",
    "            \n",
    "            if min_size >= 100 and len(counts) >= 4:\n",
    "                sil = silhouette_score(X_scaled, labels, sample_size=5000)\n",
    "                results_trans[f'dbscan_{eps:.2f}'] = {\n",
    "                    'labels': labels,\n",
    "                    'silhouette': sil,\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'min_size': min_size,\n",
    "                    'algorithm': 'dbscan'\n",
    "                }\n",
    "    \n",
    "    results_all[trans_name] = results_trans\n",
    "\n",
    "# ====================================================================\n",
    "# 4. SELE√á√ÉO DO MELHOR RESULTADO\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\nüèÜ 4. SELE√á√ÉO DO MELHOR CLUSTERING\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Encontrar melhor combina√ß√£o\n",
    "best_score = -1\n",
    "best_config = None\n",
    "all_configs = []\n",
    "\n",
    "for trans_name, results in results_all.items():\n",
    "    for config_name, config in results.items():\n",
    "        n_clusters = config['n_clusters']\n",
    "        silhouette = config.get('silhouette', 0)\n",
    "        min_size = config['min_size']\n",
    "        \n",
    "        # Score que FORTEMENTE favorece mais clusters\n",
    "        # Penaliza fortemente solu√ß√µes com poucos clusters\n",
    "        if n_clusters < 4:\n",
    "            cluster_bonus = 0.1\n",
    "        elif n_clusters < 6:\n",
    "            cluster_bonus = 0.5\n",
    "        else:\n",
    "            cluster_bonus = 1.0 + (n_clusters - 6) * 0.1\n",
    "        \n",
    "        # Aceitar silhouette mais baixo em troca de mais clusters\n",
    "        score = (silhouette + 0.3) * cluster_bonus * (min_size / 200)\n",
    "        \n",
    "        config['final_score'] = score\n",
    "        config['trans_name'] = trans_name\n",
    "        config['config_name'] = config_name\n",
    "        all_configs.append(config)\n",
    "        \n",
    "        if score > best_score and n_clusters >= 5:\n",
    "            best_score = score\n",
    "            best_config = (trans_name, config_name, config)\n",
    "\n",
    "# Se n√£o encontrou solu√ß√£o com 5+ clusters, for√ßar\n",
    "if best_config is None or best_config[2]['n_clusters'] < 5:\n",
    "    print(\"‚ö†Ô∏è For√ßando solu√ß√£o com mais clusters...\")\n",
    "    \n",
    "    # Tentar K-means com k=6 ou k=7 na representa√ß√£o combinada\n",
    "    X_scaled = StandardScaler().fit_transform(transformations['Combinado'])\n",
    "    \n",
    "    for k in [7, 6, 8, 5]:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=100, max_iter=1000, \n",
    "                       init='k-means++', random_state=42)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        min_size = min(counts)\n",
    "        \n",
    "        if min_size >= 50:  # Aceitar clusters menores\n",
    "            sil = silhouette_score(X_scaled, labels, sample_size=5000)\n",
    "            best_config = ('Combinado_Forcado', f'kmeans_{k}', {\n",
    "                'labels': labels,\n",
    "                'silhouette': sil,\n",
    "                'n_clusters': k,\n",
    "                'min_size': min_size,\n",
    "                'algorithm': 'kmeans_forced'\n",
    "            })\n",
    "            break\n",
    "\n",
    "# Mostrar top 5 configura√ß√µes\n",
    "print(\"\\nüìä Top 5 configura√ß√µes encontradas:\")\n",
    "all_configs.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "for i, cfg in enumerate(all_configs[:5]):\n",
    "    print(f\"{i+1}. {cfg['trans_name']} - {cfg['algorithm']} \"\n",
    "          f\"(K={cfg['n_clusters']}, Sil={cfg.get('silhouette', 0):.3f}, \"\n",
    "          f\"MinSize={cfg['min_size']}, Score={cfg['final_score']:.3f})\")\n",
    "\n",
    "if best_config:\n",
    "    trans_name, config_name, config = best_config\n",
    "    best_labels = config['labels']\n",
    "    n_clusters = config['n_clusters']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Configura√ß√£o selecionada:\")\n",
    "    print(f\"   Transforma√ß√£o: {trans_name}\")\n",
    "    print(f\"   Algoritmo: {config['algorithm']}\")\n",
    "    print(f\"   N√∫mero de clusters: {n_clusters}\")\n",
    "    print(f\"   Silhouette: {config.get('silhouette', 'N/A')}\")\n",
    "    print(f\"   Menor cluster: {config['min_size']:,} crian√ßas\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Nenhuma configura√ß√£o adequada encontrada!\")\n",
    "    print(\"Usando fallback com 6 clusters...\")\n",
    "    X_scaled = StandardScaler().fit_transform(X_perfis)\n",
    "    kmeans = KMeans(n_clusters=6, n_init=50, random_state=42)\n",
    "    best_labels = kmeans.fit_predict(X_scaled)\n",
    "    n_clusters = 6\n",
    "\n",
    "if best_config:\n",
    "    trans_name, config_name, config = best_config\n",
    "    best_labels = config['labels']\n",
    "    n_clusters = config['n_clusters']\n",
    "    \n",
    "    print(f\"‚úÖ Melhor configura√ß√£o encontrada:\")\n",
    "    print(f\"   Transforma√ß√£o: {trans_name}\")\n",
    "    print(f\"   Algoritmo: {config['algorithm']}\")\n",
    "    print(f\"   N√∫mero de clusters: {n_clusters}\")\n",
    "    print(f\"   Silhouette: {config['silhouette']:.3f}\")\n",
    "    print(f\"   Menor cluster: {config['min_size']:,} crian√ßas\")\n",
    "else:\n",
    "    # Fallback: for√ßar clustering com mais grupos\n",
    "    print(\"‚ö†Ô∏è For√ßando clustering com 6 grupos...\")\n",
    "    X_scaled = StandardScaler().fit_transform(X_perfis)\n",
    "    kmeans = KMeans(n_clusters=6, n_init=50, random_state=42)\n",
    "    best_labels = kmeans.fit_predict(X_scaled)\n",
    "    n_clusters = 6\n",
    "\n",
    "# ====================================================================\n",
    "# 5. AN√ÅLISE DETALHADA DOS CLUSTERS\n",
    "# ====================================================================\n",
    "\n",
    "print(f\"\\nüìä 5. AN√ÅLISE DOS {n_clusters} CLUSTERS ENCONTRADOS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Adicionar clusters ao dataset\n",
    "df_final = df_clean.copy()\n",
    "df_final['cluster'] = best_labels\n",
    "\n",
    "# Estat√≠sticas por cluster\n",
    "for cluster in sorted(np.unique(best_labels)):\n",
    "    cluster_data = df_final[df_final['cluster'] == cluster]\n",
    "    n_criancas = len(cluster_data)\n",
    "    pct = n_criancas / len(df_final) * 100\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è CLUSTER {cluster}\")\n",
    "    print(f\"   Tamanho: {n_criancas:,} crian√ßas ({pct:.1f}%)\")\n",
    "    \n",
    "    # Calcular m√©dias do cluster\n",
    "    cluster_means = cluster_data[X.columns].mean()\n",
    "    overall_means = df_final[X.columns].mean()\n",
    "    \n",
    "    # Top 10 alimentos\n",
    "    top_10 = cluster_means.nlargest(10)\n",
    "    print(\"\\n   üìà Top 10 alimentos mais consumidos:\")\n",
    "    for food, value in top_10.items():\n",
    "        diff = value - overall_means[food]\n",
    "        diff_str = f\"({diff:+.1%} vs m√©dia)\" if abs(diff) > 0.1 else \"\"\n",
    "        print(f\"      ‚Ä¢ {food}: {value:.1%} {diff_str}\")\n",
    "    \n",
    "    # Caracter√≠sticas distintivas (>20% diferen√ßa)\n",
    "    diffs = cluster_means - overall_means\n",
    "    distintivos = diffs[abs(diffs) > 0.2].sort_values(ascending=False)\n",
    "    \n",
    "    if len(distintivos) > 0:\n",
    "        print(\"\\n   ‚≠ê Caracter√≠sticas distintivas:\")\n",
    "        for food, diff in distintivos.items():\n",
    "            if diff > 0:\n",
    "                print(f\"      ‚Üë {food}: {diff:+.1%} acima da m√©dia\")\n",
    "            else:\n",
    "                print(f\"      ‚Üì {food}: {diff:.1%} abaixo da m√©dia\")\n",
    "    \n",
    "    # Perfil resumido\n",
    "    if 'e01_leite_peito' in cluster_means.index:\n",
    "        amamentacao = cluster_means['e01_leite_peito']\n",
    "        solidos = 1 - cluster_means.get('e214a_nao_comeu', 0)\n",
    "        ultraproc = cluster_means[['e30_hamburger', 'e31_salgadinhos', \n",
    "                                  'e32_suco_industrializado', 'e33_refrigerante', \n",
    "                                  'e35_biscoito', 'e36_bala']].mean()\n",
    "        \n",
    "        print(f\"\\n   üìã Perfil resumido:\")\n",
    "        print(f\"      ‚Ä¢ Amamenta√ß√£o: {amamentacao:.1%}\")\n",
    "        print(f\"      ‚Ä¢ Alimentos s√≥lidos: {solidos:.1%}\")\n",
    "        print(f\"      ‚Ä¢ Ultraprocessados: {ultraproc:.1%}\")\n",
    "\n",
    "# ====================================================================\n",
    "# 6. VISUALIZA√á√ÉO E SALVAMENTO\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\nüíæ 6. SALVANDO RESULTADOS E VISUALIZA√á√ïES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "import os\n",
    "output_path = '/Users/marcelosilva/Desktop/clustering(0-4)/4-Clustering/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Salvar resultados principais\n",
    "results_df = pd.DataFrame({\n",
    "    'id_anon': ids,\n",
    "    'cluster': best_labels\n",
    "})\n",
    "results_df.to_csv(output_path + 'clustering_multiplos_grupos.csv', index=False)\n",
    "print(f\"‚úÖ Resultado principal salvo: {n_clusters} clusters\")\n",
    "\n",
    "# Dataset completo\n",
    "df_final.to_csv(output_path + 'dataset_completo_clusters.csv', index=False)\n",
    "\n",
    "# Caracter√≠sticas por cluster\n",
    "cluster_profiles = df_final.groupby('cluster')[X.columns].mean()\n",
    "cluster_profiles.to_csv(output_path + 'perfis_clusters.csv')\n",
    "\n",
    "# Criar visualiza√ß√£o\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Distribui√ß√£o dos clusters\n",
    "plt.subplot(2, 2, 1)\n",
    "cluster_counts = df_final['cluster'].value_counts().sort_index()\n",
    "plt.bar(cluster_counts.index, cluster_counts.values)\n",
    "plt.title(f'Distribui√ß√£o dos {n_clusters} Clusters')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('N√∫mero de Crian√ßas')\n",
    "for i, v in enumerate(cluster_counts.values):\n",
    "    plt.text(i, v + 50, f'{v:,}\\n({v/len(df_final)*100:.1f}%)', \n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# Subplot 2: PCA visualization\n",
    "plt.subplot(2, 2, 2)\n",
    "pca_viz = PCA(n_components=2, random_state=42)\n",
    "X_pca_viz = pca_viz.fit_transform(X)\n",
    "scatter = plt.scatter(X_pca_viz[:, 0], X_pca_viz[:, 1], \n",
    "                     c=best_labels, cmap='tab10', alpha=0.6, s=1)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('Visualiza√ß√£o PCA dos Clusters')\n",
    "plt.xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.1%} var)')\n",
    "plt.ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.1%} var)')\n",
    "\n",
    "# Subplot 3: Heatmap dos perfis\n",
    "plt.subplot(2, 1, 2)\n",
    "# Selecionar vari√°veis mais importantes\n",
    "important_vars = []\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_means = df_final[df_final['cluster'] == cluster][X.columns].mean()\n",
    "    overall_means = df_final[X.columns].mean()\n",
    "    diffs = abs(cluster_means - overall_means)\n",
    "    important_vars.extend(diffs.nlargest(5).index.tolist())\n",
    "important_vars = list(set(important_vars))[:20]  # Top 20 vari√°veis distintivas\n",
    "\n",
    "heatmap_data = cluster_profiles[important_vars].T\n",
    "sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Propor√ß√£o de Consumo'})\n",
    "plt.title('Perfil de Consumo por Cluster (Top 20 Alimentos Distintivos)')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Alimento')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(output_path + 'visualizacao_clusters.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Visualiza√ß√µes salvas\")\n",
    "\n",
    "# Relat√≥rio resumido\n",
    "with open(output_path + 'relatorio_clusters.txt', 'w') as f:\n",
    "    f.write(f\"RELAT√ìRIO DE CLUSTERING - PADR√ïES ALIMENTARES INFANTIS\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Total de crian√ßas: {len(df_final):,}\\n\")\n",
    "    f.write(f\"N√∫mero de clusters: {n_clusters}\\n\\n\")\n",
    "    \n",
    "    for cluster in sorted(np.unique(best_labels)):\n",
    "        n = len(df_final[df_final['cluster'] == cluster])\n",
    "        f.write(f\"\\nCLUSTER {cluster}: {n:,} crian√ßas ({n/len(df_final)*100:.1f}%)\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        # Caracter√≠sticas principais\n",
    "        cluster_means = df_final[df_final['cluster'] == cluster][X.columns].mean()\n",
    "        top_5 = cluster_means.nlargest(5)\n",
    "        \n",
    "        f.write(\"Principais alimentos:\\n\")\n",
    "        for food, value in top_5.items():\n",
    "            f.write(f\"  - {food}: {value:.1%}\\n\")\n",
    "\n",
    "print(f\"\\nüéâ CLUSTERING CONCLU√çDO COM SUCESSO!\")\n",
    "print(f\"üìä {n_clusters} clusters identificados\")\n",
    "print(f\"üìÅ Resultados salvos em: {output_path}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
